# CDC Weekly Reports RAG Intelligent Questionâ€‘Answering System

<p align="center">
  <video src="demo.mp4" controls loop muted style="max-width:100%"></video>
</p>

![Demo Video](demo.gif)

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://github.com/langchain-ai/langchain)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.33+-red.svg)](https://streamlit.io/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

Built with **LangChain**, this **enterpriseâ€‘grade Retrievalâ€‘Augmented Generation (RAG) QA system** is optimised for CDC Weekly Reports data. It features a modular pipeline and integrates hybrid retrieval with smart reâ€‘ranking.

## ğŸ¯ Project Highlights

### ğŸš€ **Core Technical Highlights**

* **Modular pipeline architecture**: Indexing + Retrieval + Research subâ€‘pipeline (LangChain)
* **Hybrid retrieval optimisation**: Dense Vector Search + BM25 Sparse Search + Crossâ€‘Encoder reâ€‘ranking
* **Asynchronous parallel processing**: Multiâ€‘query parallel retrieval for faster responses
* **Cloudâ€‘native deployment**: Qdrant Cloud + Docker containers
* **Enterpriseâ€‘grade scalability**: Supports multiple vector stores (Qdrant / Elasticsearch / Pinecone / MongoDB)

### ğŸ’¡ **Intelligent QA Capabilities**

* **Domain expertise**: Fineâ€‘tuned for CDC Weekly Reports
* **Context awareness**: Coherent answers using conversation history
* **Source tracing**: Detailed citations with confidence scores
* **Multiâ€‘turn dialogue**: Handles complex interactive conversations

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚â”€â”€â”€â”€â–ºâ”‚    LangChain     â”‚â”€â”€â”€â”€â–ºâ”‚   Qdrant    â”‚
â”‚   Web UI    â”‚â—„â”€â”€â”€â”€â”‚   Orchestrator   â”‚     â”‚  Vector DB  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼         â–¼         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Nginx  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ OpenAI   â”‚   (LB)  â”‚ vLLM #1 â”‚ vLLM #2 â”‚
              â”‚ (Cloud)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚(Server) â”‚(Server) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LLM Backends
- Cloud path: direct calls to OpenAI API (default).
- Selfâ€‘hosted path: Nginx reverse proxy routes requests to two vLLM services (OpenAIâ€‘compatible API). Switch by setting `OPENAI_BASE_URL` to the Nginx endpoint.

### ğŸ”„ **Retrieval Workflow**

1. **Query understanding** â†’ analyse user intent
2. **Query expansion** â†’ generate related queries
3. **Parallel retrieval** â†’ dense & sparse search in parallel
4. **Smart reâ€‘ranking** â†’ Crossâ€‘Encoder semantic reâ€‘ranking
5. **Answer generation** â†’ craft professional answers
6. **Source attribution** â†’ attach citations

## ğŸ› ï¸ Tech Stack

| Layer               | Component               | Description                                     |
| ------------------- | ----------------------- | ----------------------------------------------- |
| **Frontend**        | Streamlit               | Modern web UI + custom CSS                      |
| **AI Framework**    | LangChain               | Runnables pipeline + async orchestration        |
| **LLM**             | OpenAI GPTâ€‘4.1          | Answer generation + query understanding         |
| **LLM Gateway**     | Nginx                   | Reverse proxy, OpenAIâ€‘compatible routing to vLLM|
| **Selfâ€‘Hosted LLM** | vLLM (x2 services)      | Onâ€‘server inference, OpenAIâ€‘compatible API      |
| **Embedding Model** | textâ€‘embeddingâ€‘adaâ€‘002  | Document vectorisation                          |
| **Vector DB**       | Qdrant Cloud            | Highâ€‘performance vector storage & search        |
| **Reâ€‘ranking**      | BAAI/bgeâ€‘rerankerâ€‘base  | Crossâ€‘Encoder semantic reâ€‘ranking               |
| **Retrieval**       | BM25 + Vector Search    | Hybrid retrieval strategy                       |
| **Deployment**      | Docker + Docker Compose | Containerised deployment                        |

## ğŸš€ Quick Start

### MethodÂ 1: Docker Deployment (recommended)

1. **Clone the project**

```bash
git clone <your-repo-url>
cd rag_0508
```

2. **Copy and edit environment variables**

```bash
cp .env.example .env
```

Fill in your keys (at minimum `OPENAI_API_KEY`, and `QDRANT_URL`/`QDRANT_API_KEY` when using Qdrant Cloud).

3. **Choose runtime mode**


*Cloud Qdrant (default)* â€” keep `COMPOSE_PROFILES=cloud` in `.env`.

*Local embedded Qdrant* â€” set `COMPOSE_PROFILES=local` in `.env` (or export it) and ensure `qdrant_data/` exists on the host.

> Use selfâ€‘hosted LLM via Nginx: point the app to your Nginx OpenAIâ€‘compatible endpoint by setting `OPENAI_BASE_URL` (e.g., `http://<nginx-host>:8081/v1`) and set `CHAT_MODEL` to the model name loaded by vLLM. Leave `OPENAI_BASE_URL` empty to use OpenAI cloud.

4. **Build and run the container**

```bash
# Cloud profile (default)
make docker-up
# or
docker compose --profile cloud up -d --build

# Local profile (embedded Qdrant)
make docker-up-local
# or
docker compose --profile local up -d --build
```

> Tip: set `PRELOAD_MODELS=1` in `.env` before building if you want the Docker image to pre-download the `BAAI/bge-reranker-base` model.

5. **Open in browser**

```
http://localhost:8501
```

Use `make docker-logs` (or `docker compose --profile cloud logs -f`) to tail the container logs. Run `make docker-down` to stop the active profile, or `DOCKER_PROFILE=local make docker-down` when the embedded Qdrant profile is running.

### MethodÂ 2: Local Development

1. **Install dependencies**

```bash
pip install -r requirements.txt
pip install -e .
```

2. **Configure environment variables** (same as above)

3. **Launch the app**

```bash
streamlit run streamlit_app.py
```

## ğŸ“‹ Features

### ğŸ¤– **Intelligent Q\&A**

* âœ… Professional CDC report analysis
* âœ… Multiâ€‘turn context understanding
* âœ… Realâ€‘time streaming responses
* âœ… Smart question recommendations

### ğŸ” **Advanced Retrieval**

* âœ… Hybrid retrieval (Dense + Sparse)
* âœ… Crossâ€‘Encoder reâ€‘ranking
* âœ… Parallel query processing
* âœ… Relevance scoring

### ğŸ“Š **User Experience**

* âœ… Modern responsive UI
* âœ… Conversation history
* âœ… Source document display
* âœ… Sample question prompts

### ğŸ›¡ï¸ **Enterpriseâ€‘Grade**

* âœ… Session persistence
* âœ… Graceful error handling
* âœ… Logging & tracing
* âœ… Health checks & monitoring

## ğŸ’» Usage Examples

### Example questions

```
ğŸ“‹ What did the CDC report about influenza activity among children?
ğŸ“‹ What did the Tennessee report say about early influenza activity in children?
ğŸ“‹ Arthritisâ€”how much higher in young male veterans?
ğŸ“‹ Why do the authors prioritise the WHO STOP AIDS package for children under 5 on ART?
```

### API interface

```python
# Retrieval chain usage
from src.retrieval_graph.graph import build_chain
from src.index_graph.graph import build_index

# Init
index = build_index()
chain = build_chain()

# Query
result = chain.invoke({
    'query': 'your question here',
    **index
})

print(result['answer'])
print(result['sources'])
```

## ğŸ”§ Configuration

### Environment variables

```bash
# Required
OPENAI_API_KEY=your-api-key
QDRANT_URL=your-qdrant-url
QDRANT_API_KEY=your-qdrant-key

# Model config
CHAT_MODEL=gpt-4.1-nano             # Or your vLLM model name
EMBEDDING_MODEL=text-embedding-ada-002

# LLM backend selection
# When set, requests go to your Nginx gateway (OpenAI-compatible) which proxies to vLLM.
# When empty or unset, requests go to OpenAI cloud.
OPENAI_BASE_URL=

# Optional
LANGSMITH_PROJECT=your-project
LANGCHAIN_TRACING_V2=1
```

### Retrieval parameter tuning

```python
# src/retrieval_graph/graph.py
TOP_K = 15      # initial retrieval count
FINAL_K = 5     # final returned count
```

## ğŸ“ˆ Performance Optimisation

### Retrieval performance

* **Parallel queries**: Multiple queries executed concurrently to minimise latency
* **Caching**: Cache vector embeddings
* **Connection pool**: Reuse database connections

### System performance

* **Asynchronous processing**: Nonâ€‘blocking I/O
* **Memory optimisation**: Stream large documents
* **Container optimisation**: Multiâ€‘stage Docker builds

## ğŸš¨ Troubleshooting

### Common issues

**1. OpenAI API error**

```bash
# Check API key
echo $OPENAI_API_KEY
# Verify model access
curl -H 'Authorization: Bearer $OPENAI_API_KEY' https://api.openai.com/v1/models
```

**2. Qdrant connection failure**

```bash
# Check network
curl -X GET $QDRANT_URL/collections -H 'api-key: $QDRANT_API_KEY'
```

**3. Docker build failure**

```bash
# Clean and rebuild
docker-compose down --volumes
docker-compose build --no-cache
docker-compose up
```

### Viewing logs

```bash
# Docker logs
docker-compose logs -f rag-app

# App logs
tail -f logs/chat_history_$(date +%Y-%m-%d).jsonl
```

## ğŸ¤ Contribution Guide

### Set up dev environment

```bash
# Install dev deps
pip install -e '.[dev]'

# Code formatting
make format

# Run tests
make test

# Lint
make lint
```



## ğŸ™ Acknowledgments

* [LangChain](https://github.com/langchain-ai/langchain) â€“ composable LLM application framework
* [Streamlit](https://streamlit.io/) â€“ elegant web app framework
* [Qdrant](https://qdrant.tech/) â€“ highâ€‘performance vector DB
* [OpenAI](https://openai.com/) â€“ advanced AI models

---

**â­ If this project is helpful, please give it a star!**
